import models
import torch
import argparse
import os
import numpy as np
import matplotlib.pyplot as plt

from tacotron2_common.utils import load_wav_to_torch
import tacotron2_common.layers as layers
import json


import umap

import dllogger as DLLogger
from dllogger import StdOutBackend, JSONStreamBackend, Verbosity



def parse_args(parser):
    """
    Parse commandline arguments.
    """
    parser.add_argument('-i', '--input', type=str, required=True,
                        help='full path to the input text (phareses separated by new line)')
    parser.add_argument('-o', '--output', required=True,
                        help='output folder to save audio (file per phrase)')
    parser.add_argument('--suffix', type=str, default="", help="output filename suffix")
    parser.add_argument('--tacotron2', type=str,
                        help='full path to the Tacotron2 model checkpoint file')
    parser.add_argument('--waveglow', type=str,
                        help='full path to the WaveGlow model checkpoint file')
    parser.add_argument('-s', '--sigma-infer', default=0.9, type=float)
    parser.add_argument('-d', '--denoising-strength', default=0.01, type=float)
    parser.add_argument('-sr', '--sampling-rate', default=22050, type=int,
                        help='Sampling rate')

    run_mode = parser.add_mutually_exclusive_group()
    run_mode.add_argument('--fp16', action='store_true',
                        help='Run inference with mixed precision')
    run_mode.add_argument('--cpu', action='store_true',
                        help='Run inference on CPU')

    parser.add_argument('--log-file', type=str, default='nvlog.json',
                        help='Filename for logging')
    parser.add_argument('--include-warmup', action='store_true',
                        help='Include warmup')
    parser.add_argument('--stft-hop-length', type=int, default=256,
                        help='STFT hop length for estimating audio length from mel size')
    parser.add_argument('--ref-path', type=str, default=None)

    return parser

def checkpoint_from_distributed(state_dict):
    """
    Checks whether checkpoint was generated by DistributedDataParallel. DDP
    wraps model in additional "module.", it needs to be unwrapped for single
    GPU inference.
    :param state_dict: model's state dict
    """
    ret = False
    for key, _ in state_dict.items():
        if key.find('module.') != -1:
            ret = True
            break
    return ret

def unwrap_distributed(state_dict):
    """
    Unwraps model from DistributedDataParallel.
    DDP wraps model in additional "module.", it needs to be removed for single
    GPU inference.
    :param state_dict: model's state dict
    """
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace('module.', '')
        new_state_dict[new_key] = value
    return new_state_dict

def load_and_setup_model(model_name, parser, checkpoint, fp16_run, cpu_run,
                         forward_is_infer=False, jittable=False):
    model_parser = models.model_parser(model_name, parser, add_help=False)
    model_args, _ = model_parser.parse_known_args()


    model_config = models.get_model_config(model_name, model_args)
    model = models.get_model(model_name, model_config, cpu_run=cpu_run,
                             forward_is_infer=forward_is_infer,
                             jittable=jittable)

    if checkpoint is not None:
        if cpu_run:
            state_dict = torch.load(checkpoint, map_location=torch.device('cpu'))['state_dict']
        else:
            state_dict = torch.load(checkpoint)['state_dict']
        if checkpoint_from_distributed(state_dict):
            state_dict = unwrap_distributed(state_dict)

        model.load_state_dict(state_dict, strict=False)

    if model_name == "WaveGlow":
        model = model.remove_weightnorm(model)

    model.eval()

    if fp16_run:
        model.half()

    return model

class MelLoader():
    def __init__(self, text_cleaners, max_wav_value, sampling_rate, filter_length, hop_length, win_length, n_mel_channels, mel_fmin, mel_fmax):
        self.text_cleaners = text_cleaners
        self.max_wav_value = max_wav_value
        self.sampling_rate = sampling_rate
        self.stft = layers.TacotronSTFT(
            filter_length, hop_length, win_length,
            n_mel_channels, sampling_rate, mel_fmin, mel_fmax)

    def get_mel(self, filename):
        audio, sampling_rate = load_wav_to_torch(filename)
        if sampling_rate != self.stft.sampling_rate:
            raise ValueError("{} {} SR doesn't match target {} SR".format(
                sampling_rate, self.stft.sampling_rate))
        audio_norm = audio / self.max_wav_value
        audio_norm = audio_norm.unsqueeze(0)
        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)
        melspec = self.stft.mel_spectrogram(audio_norm)
        melspec = torch.squeeze(melspec, 0)

        return melspec

def run(refs):
    parser = argparse.ArgumentParser(
        description='PyTorch Tacotron 2 Inference')
    parser = parse_args(parser)
    args, _ = parser.parse_known_args()

    log_file = os.path.join(args.output, args.log_file)
    DLLogger.init(backends=[JSONStreamBackend(Verbosity.DEFAULT, log_file),
                            StdOutBackend(Verbosity.VERBOSE)])
    for k,v in vars(args).items():
        DLLogger.log(step="PARAMETER", data={k:v})
    DLLogger.log(step="PARAMETER", data={'model_name':'Tacotron2_PyT'})

    tacotron2 = load_and_setup_model('Tacotron2', parser, args.tacotron2,
                                     args.fp16, args.cpu, forward_is_infer=True)


    with open('config.json') as f:
        audio_config = json.load(f)
    
    loader = MelLoader(text_cleaners=['english_cleaners'], 
                       max_wav_value=audio_config['audio']['max-wav-value'], 
                       sampling_rate=audio_config['audio']['sampling-rate'], 
                       filter_length=audio_config['audio']['filter-length'], 
                       hop_length=audio_config['audio']['hop-length'], 
                       win_length=audio_config['audio']['win-length'], 
                       n_mel_channels=80, 
                       mel_fmin=audio_config['audio']['mel-fmin'], 
                       mel_fmax=audio_config['audio']['mel-fmax'])
    
    ref_mels = []
    for ref in refs:
        ref_mel = loader.get_mel(ref)
        ref_mel = ref_mel.unsqueeze(0)
        ref_mels.append(ref_mel)


    # if not args.cpu:
    #     ref_mel = ref_mel.to('cuda')

    
    styles = []
    with torch.no_grad():
        for ref_mel in ref_mels:
            style_embed = tacotron2.gst(ref_mel.permute(0,2,1).to('cuda'))
            styles.append(style_embed)
    
    # embeddings = np.vstack(styles)
    # X_embedded = umap.UMAP(n_neighbors=5).fit_transform(embeddings)
    # plt.scatter(X_embedded[:, 0], X_embedded[:, 1])
    # plt.title("Preds UMAP")
    # plt.savefig('umap_preds.png')



if __name__ == '__main__':
    run(['/home/madusov/vkr/data/ssw_esd_22050/5_50_0.wav',
         '/home/madusov/vkr/data/ssw_esd_22050/5_50_1.wav',
         '/home/madusov/vkr/data/ssw_esd_22050/5_50_2.wav',
         '/home/madusov/vkr/data/ssw_esd_22050/5_50_3.wav',
         '/home/madusov/vkr/data/ssw_esd_22050/5_50_4.wav'])