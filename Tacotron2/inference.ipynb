{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21416447-436d-4c60-b5f6-1e52e5fdf1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madusov/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tacotron2_common.utils import load_wav_to_torch\n",
    "import tacotron2_common.layers as layers\n",
    "import json\n",
    "\n",
    "\n",
    "import umap\n",
    "\n",
    "import dllogger as DLLogger\n",
    "from dllogger import StdOutBackend, JSONStreamBackend, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b2324c-2f4b-4dd4-afde-be5f5afe6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_from_distributed(state_dict):\n",
    "    \"\"\"\n",
    "    Checks whether checkpoint was generated by DistributedDataParallel. DDP\n",
    "    wraps model in additional \"module.\", it needs to be unwrapped for single\n",
    "    GPU inference.\n",
    "    :param state_dict: model's state dict\n",
    "    \"\"\"\n",
    "    ret = False\n",
    "    for key, _ in state_dict.items():\n",
    "        if key.find('module.') != -1:\n",
    "            ret = True\n",
    "            break\n",
    "    return ret\n",
    "\n",
    "\n",
    "def unwrap_distributed(state_dict):\n",
    "    \"\"\"\n",
    "    Unwraps model from DistributedDataParallel.\n",
    "    DDP wraps model in additional \"module.\", it needs to be removed for single\n",
    "    GPU inference.\n",
    "    :param state_dict: model's state dict\n",
    "    \"\"\"\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key.replace('module.', '')\n",
    "        new_state_dict[new_key] = value\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def load_and_setup_model(model_name, parser, checkpoint, fp16_run, cpu_run,\n",
    "                         forward_is_infer=False, jittable=False):\n",
    "    model_parser = models.model_parser(model_name, parser, add_help=False)\n",
    "    model_args, _ = model_parser.parse_known_args()\n",
    "\n",
    "    model_config = models.get_model_config(model_name, model_args)\n",
    "    model = models.get_model(model_name, model_config, cpu_run=cpu_run,\n",
    "                             forward_is_infer=forward_is_infer,\n",
    "                             jittable=jittable)\n",
    "\n",
    "    if checkpoint is not None:\n",
    "        if cpu_run:\n",
    "            state_dict = torch.load(checkpoint, map_location=torch.device('cpu'))['state_dict']\n",
    "        else:\n",
    "            state_dict = torch.load(checkpoint)['state_dict']\n",
    "        if checkpoint_from_distributed(state_dict):\n",
    "            state_dict = unwrap_distributed(state_dict)\n",
    "\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    if model_name == \"WaveGlow\":\n",
    "        model = model.remove_weightnorm(model)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if fp16_run:\n",
    "        model.half()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# taken from tacotron2/data_function.py:TextMelCollate.__call__\n",
    "def pad_sequences(batch):\n",
    "    # Right zero-pad all one-hot text sequences to max input length\n",
    "    input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "        torch.LongTensor([len(x) for x in batch]),\n",
    "        dim=0, descending=True)\n",
    "    max_input_len = input_lengths[0]\n",
    "\n",
    "    text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "    text_padded.zero_()\n",
    "    for i in range(len(ids_sorted_decreasing)):\n",
    "        text = batch[ids_sorted_decreasing[i]]\n",
    "        text_padded[i, :text.size(0)] = text\n",
    "\n",
    "    return text_padded, input_lengths\n",
    "\n",
    "\n",
    "def prepare_input_sequence(texts, cpu_run=False):\n",
    "    emotions = {}\n",
    "    d = []\n",
    "    for i,text in enumerate(texts):\n",
    "        flag = True\n",
    "        while flag:\n",
    "            flag = False\n",
    "            for emo in tag2ref.keys():\n",
    "                pos = text.find(emo)\n",
    "                if pos != -1:\n",
    "                    text = text.replace(emo, '', 1)\n",
    "                    if emo in emotions:\n",
    "                        emotions[emo].append(pos)\n",
    "                    else:\n",
    "                        emotions[emo] = [pos]\n",
    "                    flag = True\n",
    "\n",
    "        d.append(torch.IntTensor(\n",
    "            text_to_sequence(text, ['english_cleaners'])[:]))\n",
    "\n",
    "    text_padded, input_lengths = pad_sequences(d)\n",
    "    if not cpu_run:\n",
    "        text_padded = text_padded.cuda().long()\n",
    "        input_lengths = input_lengths.cuda().long()\n",
    "    else:\n",
    "        text_padded = text_padded.long()\n",
    "        input_lengths = input_lengths.long()\n",
    "\n",
    "    return text_padded, input_lengths, emotions\n",
    "\n",
    "class MelLoader():\n",
    "    def __init__(self, text_cleaners, max_wav_value, sampling_rate, filter_length, hop_length, win_length, n_mel_channels, mel_fmin, mel_fmax, segment_length=None):\n",
    "        self.text_cleaners = text_cleaners\n",
    "        self.max_wav_value = max_wav_value\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.segment_length = segment_length\n",
    "        self.stft = layers.TacotronSTFT(\n",
    "            filter_length, hop_length, win_length,\n",
    "            n_mel_channels, sampling_rate, mel_fmin, mel_fmax)\n",
    "\n",
    "    def get_mel(self, filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename)\n",
    "        if sampling_rate != self.stft.sampling_rate:\n",
    "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "                sampling_rate, self.stft.sampling_rate))\n",
    "        audio_norm = audio / self.max_wav_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
    "        melspec = torch.squeeze(melspec, 0)\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mel_audio(self, filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename)\n",
    "        if sampling_rate != self.stft.sampling_rate:\n",
    "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "                sampling_rate, self.stft.sampling_rate))\n",
    "        \n",
    "        # Take segment\n",
    "        if audio.size(0) >= self.segment_length:\n",
    "            max_audio_start = audio.size(0) - self.segment_length\n",
    "            audio_start = torch.randint(0, max_audio_start + 1, size=(1,)).item()\n",
    "            audio_segment = audio[audio_start:audio_start+self.segment_length]\n",
    "        else:\n",
    "            audio_segment = torch.nn.functional.pad(\n",
    "                audio, (0, self.segment_length - audio.size(0)), 'constant').data\n",
    "        audio_segment = audio_segment / self.max_wav_value\n",
    "\n",
    "        audio_norm = audio / self.max_wav_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
    "        melspec = torch.squeeze(melspec, 0)\n",
    "        \n",
    "        return melspec, audio_segment\n",
    "\n",
    "class MeasureTime():\n",
    "    def __init__(self, measurements, key, cpu_run=False):\n",
    "        self.measurements = measurements\n",
    "        self.key = key\n",
    "        self.cpu_run = cpu_run\n",
    "\n",
    "    def __enter__(self):\n",
    "        if not self.cpu_run:\n",
    "            torch.cuda.synchronize()\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        if not self.cpu_run:\n",
    "            torch.cuda.synchronize()\n",
    "        self.measurements[self.key] = time.perf_counter() - self.t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fada8c31-2a0b-47d1-bb91-5de4b09100f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tacotron_path = 'output/checkpoint_Tacotron2_6420.pt'\n",
    "\n",
    "class Args(argparse.Namespace):\n",
    "    input='phrases/phrase.txt'\n",
    "    output='output/'\n",
    "    suffix=''\n",
    "    tacotron2=tacotron_path\n",
    "    waveglow='checkpoints/waveglow_1076430_14000_amp'\n",
    "    sigma_infer=0.9\n",
    "    denoising_strength=0.01\n",
    "    sampling_rate=22050\n",
    "    fp16=False\n",
    "    cpu=False\n",
    "    log_file='nvlog.json'\n",
    "    include_warmup=False\n",
    "    stft_hop_length=256\n",
    "    ref_path='/home/madusov/vkr/data/ssw_esd_ljspeech_22050/wavs/5_50_2.wav'\n",
    "    \n",
    "class Args2(argparse.Namespace):\n",
    "    input='phrases/phrase.txt'\n",
    "    output='output/'\n",
    "    suffix=''\n",
    "    tacotron2=tacotron_path\n",
    "    waveglow='checkpoints/waveglow_1076430_14000_amp'\n",
    "    sigma_infer=0.9\n",
    "    denoising_strength=0.01, \n",
    "    sampling_rate=22050, \n",
    "    fp16=False, \n",
    "    cpu=False \n",
    "    log_file='nvlog.json'\n",
    "    include_warmup=False \n",
    "    stft_hop_length=256 \n",
    "    ref_path='/home/madusov/vkr/data/ssw_esd_ljspeech_22050/wavs/5_50_2.wav'\n",
    "    mask_padding=False\n",
    "    n_mel_channels=80 \n",
    "    n_symbols=148\n",
    "    symbols_embedding_dim=512\n",
    "    encoder_kernel_size=5\n",
    "    encoder_n_convolutions=3\n",
    "    encoder_embedding_dim=512\n",
    "    n_frames_per_step=1\n",
    "    decoder_rnn_dim=1024\n",
    "    prenet_dim=256\n",
    "    max_decoder_steps=2000\n",
    "    gate_threshold=0.5\n",
    "    p_attention_dropout=0.1\n",
    "    p_decoder_dropout=0.1\n",
    "    decoder_no_early_stopping=False\n",
    "    attention_rnn_dim=1024\n",
    "    attention_dim=128\n",
    "    attention_location_n_filters=32\n",
    "    attention_location_kernel_size=31 \n",
    "    postnet_embedding_dim=512\n",
    "    postnet_kernel_size=5\n",
    "    postnet_n_convolutions=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1647820-07d4-41cf-9b6a-980a1bfe2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args_model = Args2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5314cece-5669-43e9-85cc-0d08c9d14531",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = os.path.join(args.output, args.log_file)\n",
    "DLLogger.init(backends=[JSONStreamBackend(Verbosity.DEFAULT, log_file),\n",
    "                        StdOutBackend(Verbosity.VERBOSE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e65c260-5c51-42b1-87e1-c8ffe991f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLL 2025-05-04 20:40:44.584222 - PARAMETER model_name : Tacotron2_PyT \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m DLLogger\u001b[38;5;241m.\u001b[39mlog(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPARAMETER\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTacotron2_PyT\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      5\u001b[0m args\u001b[38;5;241m.\u001b[39msegment_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[0;32m----> 6\u001b[0m tacotron2 \u001b[38;5;241m=\u001b[39m load_and_setup_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTacotron2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mparser\u001b[49m, args\u001b[38;5;241m.\u001b[39mtacotron2,\n\u001b[1;32m      7\u001b[0m                                  args\u001b[38;5;241m.\u001b[39mfp16, args\u001b[38;5;241m.\u001b[39mcpu, forward_is_infer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m waveglow \u001b[38;5;241m=\u001b[39m load_and_setup_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWaveGlow\u001b[39m\u001b[38;5;124m'\u001b[39m, parser, args\u001b[38;5;241m.\u001b[39mwaveglow,\n\u001b[1;32m      9\u001b[0m                                 args\u001b[38;5;241m.\u001b[39mfp16, args\u001b[38;5;241m.\u001b[39mcpu, forward_is_infer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m                                 jittable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m denoiser \u001b[38;5;241m=\u001b[39m Denoiser(waveglow)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "for k,v in vars(args).items():\n",
    "    DLLogger.log(step=\"PARAMETER\", data={k:v})\n",
    "DLLogger.log(step=\"PARAMETER\", data={'model_name':'Tacotron2_PyT'})\n",
    "\n",
    "args.segment_length = 50000\n",
    "tacotron2 = load_and_setup_model('Tacotron2', parser, args.tacotron2,\n",
    "                                 args.fp16, args.cpu, forward_is_infer=True)\n",
    "\n",
    "# load tacotron\n",
    "model_config = models.get_model_config('Tacotron2', args_model)\n",
    "model = models.get_model('Tacotron2', model_config, cpu_run=cpu_run,\n",
    "                         forward_is_infer=forward_is_infer,\n",
    "                         jittable=jittable)\n",
    "\n",
    "if checkpoint is not None:\n",
    "    if cpu_run:\n",
    "        state_dict = torch.load(checkpoint, map_location=torch.device('cpu'))['state_dict']\n",
    "    else:\n",
    "        state_dict = torch.load(checkpoint)['state_dict']\n",
    "    if checkpoint_from_distributed(state_dict):\n",
    "        state_dict = unwrap_distributed(state_dict)\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "\n",
    "waveglow = load_and_setup_model('WaveGlow', parser, args.waveglow,\n",
    "                                args.fp16, args.cpu, forward_is_infer=True,\n",
    "                                jittable=True)\n",
    "denoiser = Denoiser(waveglow)\n",
    "if not args.cpu:\n",
    "    denoiser.cuda()\n",
    "\n",
    "waveglow.make_ts_scriptable()\n",
    "jitted_waveglow = torch.jit.script(waveglow)\n",
    "# jitted_tacotron2 = torch.jit.script(tacotron2)\n",
    "\n",
    "texts = []\n",
    "try:\n",
    "    f = open(args.input, 'r')\n",
    "    texts = f.readlines()\n",
    "except:\n",
    "    print(\"Could not read file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "with open('config.json') as f:\n",
    "    audio_config = json.load(f)\n",
    "\n",
    "loader = MelLoader(text_cleaners=['english_cleaners'], \n",
    "                   max_wav_value=audio_config['audio']['max-wav-value'], \n",
    "                   sampling_rate=audio_config['audio']['sampling-rate'], \n",
    "                   filter_length=audio_config['audio']['filter-length'], \n",
    "                   hop_length=audio_config['audio']['hop-length'], \n",
    "                   win_length=audio_config['audio']['win-length'], \n",
    "                   n_mel_channels=80, \n",
    "                   mel_fmin=audio_config['audio']['mel-fmin'], \n",
    "                   mel_fmax=audio_config['audio']['mel-fmax'],\n",
    "                   segment_length=args.segment_length)\n",
    "# ref_mel = loader.get_mel(args.ref_path)\n",
    "# ref_mel = ref_mel.unsqueeze(0)\n",
    "\n",
    "# load emotion mels\n",
    "for emo, path in tag2ref.items():\n",
    "    emo_mel, emo_audio_segment = loader.get_mel_audio(path)\n",
    "    emo_mel = emo_mel.unsqueeze(0)\n",
    "\n",
    "    if not args.cpu:\n",
    "        emo_mel = emo_mel.to('cuda')\n",
    "\n",
    "    tag2ref[emo] = {'mel': emo_mel, 'audio': emo_audio_segment}\n",
    "\n",
    "# if not args.cpu:\n",
    "#     ref_mel = ref_mel.to('cuda')\n",
    "\n",
    "\n",
    "# if args.include_warmup:\n",
    "#     sequence = torch.randint(low=0, high=148, size=(1,50)).long()\n",
    "#     input_lengths = torch.IntTensor([sequence.size(1)]).long()\n",
    "#     if not args.cpu:\n",
    "#         sequence = sequence.cuda()\n",
    "#         input_lengths = input_lengths.cuda()\n",
    "#     for i in range(3):\n",
    "#         with torch.no_grad():\n",
    "#             mel, mel_lengths, _ = tacotron2(sequence, input_lengths)\n",
    "#             _ = jitted_waveglow(mel)\n",
    "\n",
    "measurements = {}\n",
    "\n",
    "sequences_padded, input_lengths, emotions = prepare_input_sequence(texts, args.cpu)\n",
    "\n",
    "for emo, positions in emotions.items():\n",
    "    emotions[emo] = {'pos': positions, 'mel': tag2ref[emo]['mel'], 'audio': tag2ref[emo]['audio']}\n",
    "\n",
    "if '<NEUTRAL>' not in emotions:\n",
    "    emotions['<NEUTRAL>'] = {'pos': [], 'mel': tag2ref['<NEUTRAL>']['mel'], 'audio': tag2ref['<NEUTRAL>']['audio']}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d97f92-54d5-4098-890c-013c75011ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), MeasureTime(measurements, \"tacotron2_time\", args.cpu):\n",
    "    mel, mel_lengths, alignments = tacotron2(sequences_padded, input_lengths, emotions)\n",
    "\n",
    "with torch.no_grad(), MeasureTime(measurements, \"waveglow_time\", args.cpu):\n",
    "    audios = jitted_waveglow(mel, sigma=args.sigma_infer)\n",
    "    audios = audios.float()\n",
    "with torch.no_grad(), MeasureTime(measurements, \"denoiser_time\", args.cpu):\n",
    "    audios = denoiser(audios, strength=args.denoising_strength).squeeze(1)\n",
    "\n",
    "print(\"Stopping after\",mel.size(2),\"decoder steps\")\n",
    "tacotron2_infer_perf = mel.size(0)*mel.size(2)/measurements['tacotron2_time']\n",
    "waveglow_infer_perf = audios.size(0)*audios.size(1)/measurements['waveglow_time']\n",
    "\n",
    "DLLogger.log(step=0, data={\"tacotron2_items_per_sec\": tacotron2_infer_perf})\n",
    "DLLogger.log(step=0, data={\"tacotron2_latency\": measurements['tacotron2_time']})\n",
    "DLLogger.log(step=0, data={\"waveglow_items_per_sec\": waveglow_infer_perf})\n",
    "DLLogger.log(step=0, data={\"waveglow_latency\": measurements['waveglow_time']})\n",
    "DLLogger.log(step=0, data={\"denoiser_latency\": measurements['denoiser_time']})\n",
    "DLLogger.log(step=0, data={\"latency\": (measurements['tacotron2_time']+measurements['waveglow_time']+measurements['denoiser_time'])})\n",
    "\n",
    "for i, audio in enumerate(audios):\n",
    "\n",
    "    plt.imshow(alignments[i].float().data.cpu().numpy().T, aspect=\"auto\", origin=\"lower\")\n",
    "    figure_path = os.path.join(args.output,\"alignment_\"+str(i)+args.suffix+\".png\")\n",
    "    plt.savefig(figure_path)\n",
    "\n",
    "    audio = audio[:mel_lengths[i]*args.stft_hop_length]\n",
    "    audio = audio/torch.max(torch.abs(audio))\n",
    "    audio_path = os.path.join(args.output,\"audio_\"+str(i)+args.suffix+\".wav\")\n",
    "    write(audio_path, args.sampling_rate, audio.cpu().numpy())\n",
    "\n",
    "DLLogger.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (taco_nv)",
   "language": "python",
   "name": "taco_nv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
