# *****************************************************************************
#  Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions are met:
#      * Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#      * Redistributions in binary form must reproduce the above copyright
#        notice, this list of conditions and the following disclaimer in the
#        documentation and/or other materials provided with the distribution.
#      * Neither the name of the NVIDIA CORPORATION nor the
#        names of its contributors may be used to endorse or promote products
#        derived from this software without specific prior written permission.
#
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
#  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
#  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
#  DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
#  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
#  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
# *****************************************************************************

from tacotron2.text import text_to_sequence
import models
import torch
import argparse
import os
import numpy as np
from scipy.io.wavfile import write
import matplotlib
import matplotlib.pyplot as plt

import librosa
from tacotron2.data_function import TextMelLoader
from tacotron2_common.utils import load_wav_to_torch
import tacotron2_common.layers as layers
import json


import sys

import time
import dllogger as DLLogger
from dllogger import StdOutBackend, JSONStreamBackend, Verbosity

from waveglow.denoiser import Denoiser

# ok for output_old/checkpoint_Tacotron2_6970.pt
# tag2ref = {
#     "<ANGRY>": '/home/madusov/vkr/data/ssw_esd_22050/5_38_1.wav',
#     "<SAD>": '/home/madusov/vkr/data/ssw_esd_22050/5_50_3.wav',
#     "<NEUTRAL>": '/home/madusov/vkr/data/ssw_esd_22050/5_50_0.wav',
#     "<HAPPY>": '/home/madusov/vkr/data/ssw_esd_22050/5_50_2.wav',
#     "<SURPRISED>": "/home/madusov/vkr/data/ssw_esd_22050/5_50_4.wav"
# }

tag2ref = {
    "<Angry>": '/home/madusov/vkr/data/ssw_esd_22050/5_38_1.wav',
    "<Sad>": '/home/madusov/vkr/data/ssw_esd_22050/5_38_3.wav',
    "<Neutral>": '/home/madusov/vkr/data/ssw_esd_22050/5_50_0.wav',
    "<Happy>": '/home/madusov/vkr/data/ssw_esd_22050/5_38_2.wav',
    "<Surprised>": "/home/madusov/vkr/data/ssw_esd_22050/5_50_4.wav"
}

def parse_args(parser):
    """
    Parse commandline arguments.
    """
    parser.add_argument('-i', '--input', type=str, required=True,
                        help='full path to the input text (phareses separated by new line)')
    parser.add_argument('-o', '--output', required=True,
                        help='output folder to save audio (file per phrase)')
    parser.add_argument('--suffix', type=str, default="", help="output filename suffix")
    parser.add_argument('--tacotron2', type=str,
                        help='full path to the Tacotron2 model checkpoint file')
    parser.add_argument('--waveglow', type=str,
                        help='full path to the WaveGlow model checkpoint file')
    parser.add_argument('-s', '--sigma-infer', default=0.9, type=float)
    parser.add_argument('-d', '--denoising-strength', default=0.01, type=float)
    parser.add_argument('-sr', '--sampling-rate', default=22050, type=int,
                        help='Sampling rate')

    run_mode = parser.add_mutually_exclusive_group()
    run_mode.add_argument('--fp16', action='store_true',
                        help='Run inference with mixed precision')
    run_mode.add_argument('--cpu', action='store_true',
                        help='Run inference on CPU')

    parser.add_argument('--log-file', type=str, default='nvlog.json',
                        help='Filename for logging')
    parser.add_argument('--include-warmup', action='store_true',
                        help='Include warmup')
    parser.add_argument('--stft-hop-length', type=int, default=256,
                        help='STFT hop length for estimating audio length from mel size')
    # parser.add_argument('--ref-path', type=str, default=None)

    return parser


def checkpoint_from_distributed(state_dict):
    """
    Checks whether checkpoint was generated by DistributedDataParallel. DDP
    wraps model in additional "module.", it needs to be unwrapped for single
    GPU inference.
    :param state_dict: model's state dict
    """
    ret = False
    for key, _ in state_dict.items():
        if key.find('module.') != -1:
            ret = True
            break
    return ret


def unwrap_distributed(state_dict):
    """
    Unwraps model from DistributedDataParallel.
    DDP wraps model in additional "module.", it needs to be removed for single
    GPU inference.
    :param state_dict: model's state dict
    """
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace('module.', '')
        new_state_dict[new_key] = value
    return new_state_dict


def load_and_setup_model(model_name, parser, checkpoint, fp16_run, cpu_run,
                         forward_is_infer=False, jittable=False):
    model_parser = models.model_parser(model_name, parser, add_help=False)
    model_args, _ = model_parser.parse_known_args()

    model_config = models.get_model_config(model_name, model_args)
    model = models.get_model(model_name, model_config, cpu_run=cpu_run,
                             forward_is_infer=forward_is_infer,
                             jittable=jittable)

    if checkpoint is not None:
        if cpu_run:
            state_dict = torch.load(checkpoint, map_location=torch.device('cpu'))['state_dict']
        else:
            state_dict = torch.load(checkpoint)['state_dict']
        if checkpoint_from_distributed(state_dict):
            state_dict = unwrap_distributed(state_dict)

        model.load_state_dict(state_dict, strict=True)

    if model_name == "WaveGlow":
        model = model.remove_weightnorm(model)

    model.eval()

    if fp16_run:
        model.half()

    return model


# taken from tacotron2/data_function.py:TextMelCollate.__call__
def pad_sequences(batch, emotions):
    # Right zero-pad all one-hot text sequences to max input length
    input_lengths, ids_sorted_decreasing = torch.sort(
        torch.LongTensor([len(x) for x in batch]),
        dim=0, descending=True)
    max_input_len = input_lengths[0]

    text_padded = torch.LongTensor(len(batch), max_input_len)
    text_padded.zero_()
    emotions_sorted = []
    for i in range(len(ids_sorted_decreasing)):
        text = batch[ids_sorted_decreasing[i]]
        text_padded[i, :text.size(0)] = text
        emotions_sorted.append(emotions[ids_sorted_decreasing[i]])

    return text_padded, input_lengths, emotions_sorted


def prepare_input_sequence(texts, cpu_run=False):
    emotions_all = []
    d = []
    for i,text in enumerate(texts):
        emotions = {}
        flag = True
        while flag:
            flag = False
            for emo in tag2ref.keys():
                pos = text.find(emo)
                if pos != -1:
                    text = text.replace(emo, '', 1)
                    if emo in emotions:
                        emotions[emo].append(pos)
                    else:
                        emotions[emo] = [pos]
                    flag = True
        emotions_all.append(emotions)

        d.append(torch.IntTensor(
            text_to_sequence(text, ['english_cleaners'])[:]))

    text_padded, input_lengths, emotions_all = pad_sequences(d, emotions_all)
    if not cpu_run:
        text_padded = text_padded.cuda().long()
        input_lengths = input_lengths.cuda().long()
    else:
        text_padded = text_padded.long()
        input_lengths = input_lengths.long()

    return text_padded, input_lengths, emotions_all

class MelLoader():
    def __init__(self, text_cleaners, max_wav_value, sampling_rate, filter_length, hop_length, win_length, n_mel_channels, mel_fmin, mel_fmax, segment_length=None):
        self.text_cleaners = text_cleaners
        self.max_wav_value = max_wav_value
        self.sampling_rate = sampling_rate
        self.segment_length = segment_length
        self.stft = layers.TacotronSTFT(
            filter_length, hop_length, win_length,
            n_mel_channels, sampling_rate, mel_fmin, mel_fmax)

    def get_mel(self, filename):
        audio, sampling_rate = load_wav_to_torch(filename)
        if sampling_rate != self.stft.sampling_rate:
            raise ValueError("{} {} SR doesn't match target {} SR".format(
                sampling_rate, self.stft.sampling_rate))
        audio_norm = audio / self.max_wav_value
        audio_norm = audio_norm.unsqueeze(0)
        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)
        melspec = self.stft.mel_spectrogram(audio_norm)
        melspec = torch.squeeze(melspec, 0)

        return melspec
    
    def get_mel_audio(self, filename):
        audio, sampling_rate = load_wav_to_torch(filename)
        if sampling_rate != self.stft.sampling_rate:
            raise ValueError("{} {} SR doesn't match target {} SR".format(
                sampling_rate, self.stft.sampling_rate))
        
        # Take segment
        if audio.size(0) >= self.segment_length:
            max_audio_start = audio.size(0) - self.segment_length
            audio_start = torch.randint(0, max_audio_start + 1, size=(1,)).item()
            audio_segment = audio[audio_start:audio_start+self.segment_length]
        else:
            audio_segment = torch.nn.functional.pad(
                audio, (0, self.segment_length - audio.size(0)), 'constant').data
        audio_segment = audio_segment / self.max_wav_value

        audio_norm = audio / self.max_wav_value
        audio_norm = audio_norm.unsqueeze(0)
        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)
        melspec = self.stft.mel_spectrogram(audio_norm)
        melspec = torch.squeeze(melspec, 0)
        
        return melspec, audio_segment

class MeasureTime():
    def __init__(self, measurements, key, cpu_run=False):
        self.measurements = measurements
        self.key = key
        self.cpu_run = cpu_run

    def __enter__(self):
        if not self.cpu_run:
            torch.cuda.synchronize()
        self.t0 = time.perf_counter()

    def __exit__(self, exc_type, exc_value, exc_traceback):
        if not self.cpu_run:
            torch.cuda.synchronize()
        self.measurements[self.key] = time.perf_counter() - self.t0


def main_gst():
    """
    Launches text to speech (inference) by batches.
    Inference is executed on a single GPU or CPU.
    """
    parser = argparse.ArgumentParser(
        description='PyTorch Tacotron 2 Inference')
    parser = parse_args(parser)
    args, _ = parser.parse_known_args()

    log_file = os.path.join(args.output, args.log_file)
    DLLogger.init(backends=[JSONStreamBackend(Verbosity.DEFAULT, log_file),
                            StdOutBackend(Verbosity.VERBOSE)])
    for k,v in vars(args).items():
        DLLogger.log(step="PARAMETER", data={k:v})
    DLLogger.log(step="PARAMETER", data={'model_name':'Tacotron2_PyT'})

    tacotron2 = load_and_setup_model('Tacotron2', parser, args.tacotron2,
                                     args.fp16, args.cpu, forward_is_infer=True)
    waveglow = load_and_setup_model('WaveGlow', parser, args.waveglow,
                                    args.fp16, args.cpu, forward_is_infer=True,
                                    jittable=True)
    denoiser = Denoiser(waveglow)
    if not args.cpu:
        denoiser.cuda()

    waveglow.make_ts_scriptable()
    jitted_waveglow = torch.jit.script(waveglow)
    # jitted_tacotron2 = torch.jit.script(tacotron2)

    texts = []
    try:
        f = open(args.input, 'r')
        texts = f.readlines()
    except:
        print("Could not read file")
        sys.exit(1)

    with open('config.json') as f:
        audio_config = json.load(f)
    
    loader = MelLoader(text_cleaners=['english_cleaners'], 
                       max_wav_value=audio_config['audio']['max-wav-value'], 
                       sampling_rate=audio_config['audio']['sampling-rate'], 
                       filter_length=audio_config['audio']['filter-length'], 
                       hop_length=audio_config['audio']['hop-length'], 
                       win_length=audio_config['audio']['win-length'], 
                       n_mel_channels=80, 
                       mel_fmin=audio_config['audio']['mel-fmin'], 
                       mel_fmax=audio_config['audio']['mel-fmax'])

    # load emotion mels
    for emo, path in tag2ref.items():
        emo_mel = loader.get_mel(path)
        emo_mel = emo_mel.unsqueeze(0)

        if not args.cpu:
            emo_mel = emo_mel.to('cuda')

        tag2ref[emo] = emo_mel


    measurements = {}

    sequences_padded, input_lengths, emotions = prepare_input_sequence(texts, args.cpu)

    print(emotions)
    for text_emotions in emotions:
        for emo, positions in text_emotions.items():
            text_emotions[emo] = {'pos': positions, 'mel': tag2ref[emo]}

    for text_emotions in emotions:
        if '<Neutral>' not in text_emotions:
            text_emotions['<Neutral>'] = {'pos': [], 'mel': tag2ref['<Neutral>']}

    with torch.no_grad(), MeasureTime(measurements, "tacotron2_time", args.cpu):
        mel, mel_lengths, alignments = tacotron2(sequences_padded, input_lengths, emotions)#, ref_mel_2)

    with torch.no_grad(), MeasureTime(measurements, "waveglow_time", args.cpu):
        audios = jitted_waveglow(mel, sigma=args.sigma_infer)
        audios = audios.float()
    with torch.no_grad(), MeasureTime(measurements, "denoiser_time", args.cpu):
        audios = denoiser(audios, strength=args.denoising_strength).squeeze(1)

    print("Stopping after",mel.size(2),"decoder steps")
    tacotron2_infer_perf = mel.size(0)*mel.size(2)/measurements['tacotron2_time']
    waveglow_infer_perf = audios.size(0)*audios.size(1)/measurements['waveglow_time']

    DLLogger.log(step=0, data={"tacotron2_items_per_sec": tacotron2_infer_perf})
    DLLogger.log(step=0, data={"tacotron2_latency": measurements['tacotron2_time']})
    DLLogger.log(step=0, data={"waveglow_items_per_sec": waveglow_infer_perf})
    DLLogger.log(step=0, data={"waveglow_latency": measurements['waveglow_time']})
    DLLogger.log(step=0, data={"denoiser_latency": measurements['denoiser_time']})
    DLLogger.log(step=0, data={"latency": (measurements['tacotron2_time']+measurements['waveglow_time']+measurements['denoiser_time'])})

    for i, audio in enumerate(audios):

        plt.imshow(alignments[i].float().data.cpu().numpy().T, aspect="auto", origin="lower")
        figure_path = os.path.join(args.output,"alignment_"+str(i)+args.suffix+".png")
        plt.savefig(figure_path)

        audio = audio[:mel_lengths[i]*args.stft_hop_length]
        audio = audio/torch.max(torch.abs(audio))
        audio_path = os.path.join(args.output,"audio_"+str(i)+args.suffix+".wav")
        write(audio_path, args.sampling_rate, audio.cpu().numpy())

    DLLogger.flush()


def main_gst_by_one():
    """
    Launches text to speech (inference) by one.
    Inference is executed on a single GPU or CPU.
    """
    parser = argparse.ArgumentParser(
        description='PyTorch Tacotron 2 Inference')
    parser = parse_args(parser)
    args, _ = parser.parse_known_args()

    log_file = os.path.join(args.output, args.log_file)
    DLLogger.init(backends=[JSONStreamBackend(Verbosity.DEFAULT, log_file),
                            StdOutBackend(Verbosity.VERBOSE)])
    for k,v in vars(args).items():
        DLLogger.log(step="PARAMETER", data={k:v})
    DLLogger.log(step="PARAMETER", data={'model_name':'Tacotron2_PyT'})

    tacotron2 = load_and_setup_model('Tacotron2', parser, args.tacotron2,
                                     args.fp16, args.cpu, forward_is_infer=True)
    waveglow = load_and_setup_model('WaveGlow', parser, args.waveglow,
                                    args.fp16, args.cpu, forward_is_infer=True,
                                    jittable=True)
    denoiser = Denoiser(waveglow)
    if not args.cpu:
        denoiser.cuda()

    waveglow.make_ts_scriptable()
    jitted_waveglow = torch.jit.script(waveglow)
    # jitted_tacotron2 = torch.jit.script(tacotron2)

    texts = []
    try:
        f = open(args.input, 'r')
        texts = f.readlines()
    except:
        print("Could not read file")
        sys.exit(1)

    with open('config.json') as f:
        audio_config = json.load(f)
    
    loader = MelLoader(text_cleaners=['english_cleaners'], 
                       max_wav_value=audio_config['audio']['max-wav-value'], 
                       sampling_rate=audio_config['audio']['sampling-rate'], 
                       filter_length=audio_config['audio']['filter-length'], 
                       hop_length=audio_config['audio']['hop-length'], 
                       win_length=audio_config['audio']['win-length'], 
                       n_mel_channels=80, 
                       mel_fmin=audio_config['audio']['mel-fmin'], 
                       mel_fmax=audio_config['audio']['mel-fmax'])

    # load emotion mels
    for emo, path in tag2ref.items():
        emo_mel = loader.get_mel(path)
        emo_mel = emo_mel.unsqueeze(0)

        if not args.cpu:
            emo_mel = emo_mel.to('cuda')

        tag2ref[emo] = emo_mel

    measurements = {}

    for i, text in enumerate(texts):
        print(text)
        sequences_padded, input_lengths, emotions = prepare_input_sequence([text], args.cpu)

        for text_emotions in emotions:
            for emo, positions in text_emotions.items():
                text_emotions[emo] = {'pos': positions, 'mel': tag2ref[emo]}

        for text_emotions in emotions:
            if '<Neutral>' not in text_emotions:
                text_emotions['<Neutral>'] = {'pos': [], 'mel': tag2ref['<Neutral>']}

        with torch.no_grad(), MeasureTime(measurements, "tacotron2_time", args.cpu):
            mel, mel_lengths, alignments = tacotron2(sequences_padded, input_lengths, emotions)#, ref_mel_2)

        with torch.no_grad(), MeasureTime(measurements, "waveglow_time", args.cpu):
            audios = jitted_waveglow(mel, sigma=args.sigma_infer)
            audios = audios.float()
        with torch.no_grad(), MeasureTime(measurements, "denoiser_time", args.cpu):
            audios = denoiser(audios, strength=args.denoising_strength).squeeze(1)

        print("Stopping after",mel.size(2),"decoder steps")
        tacotron2_infer_perf = mel.size(0)*mel.size(2)/measurements['tacotron2_time']
        waveglow_infer_perf = audios.size(0)*audios.size(1)/measurements['waveglow_time']

        DLLogger.log(step=0, data={"tacotron2_items_per_sec": tacotron2_infer_perf})
        DLLogger.log(step=0, data={"tacotron2_latency": measurements['tacotron2_time']})
        DLLogger.log(step=0, data={"waveglow_items_per_sec": waveglow_infer_perf})
        DLLogger.log(step=0, data={"waveglow_latency": measurements['waveglow_time']})
        DLLogger.log(step=0, data={"denoiser_latency": measurements['denoiser_time']})
        DLLogger.log(step=0, data={"latency": (measurements['tacotron2_time']+measurements['waveglow_time']+measurements['denoiser_time'])})

        assert len(audios) == 1
        audio = audios[0]
        plt.imshow(alignments[0].float().data.cpu().numpy().T, aspect="auto", origin="lower")
        figure_path = os.path.join(args.output,"alignment_"+str(i)+args.suffix+".png")
        plt.savefig(figure_path)

        audio = audio[:mel_lengths[0]*args.stft_hop_length]
        audio = audio/torch.max(torch.abs(audio))
        audio_path = os.path.join(args.output,"audio_"+str(i)+args.suffix+".wav")
        write(audio_path, args.sampling_rate, audio.cpu().numpy())

    DLLogger.flush()


def main_gst_intermediate():
    """
    Check emotion interpolation.
    Inference is executed on a single GPU or CPU.
    """
    parser = argparse.ArgumentParser(
        description='PyTorch Tacotron 2 Inference')
    parser = parse_args(parser)
    args, _ = parser.parse_known_args()

    log_file = os.path.join(args.output, args.log_file)
    DLLogger.init(backends=[JSONStreamBackend(Verbosity.DEFAULT, log_file),
                            StdOutBackend(Verbosity.VERBOSE)])
    for k,v in vars(args).items():
        DLLogger.log(step="PARAMETER", data={k:v})
    DLLogger.log(step="PARAMETER", data={'model_name':'Tacotron2_PyT'})

    tacotron2 = load_and_setup_model('Tacotron2', parser, args.tacotron2,
                                     args.fp16, args.cpu, forward_is_infer=True)
    waveglow = load_and_setup_model('WaveGlow', parser, args.waveglow,
                                    args.fp16, args.cpu, forward_is_infer=True,
                                    jittable=True)
    denoiser = Denoiser(waveglow)
    if not args.cpu:
        denoiser.cuda()

    waveglow.make_ts_scriptable()
    jitted_waveglow = torch.jit.script(waveglow)
    # jitted_tacotron2 = torch.jit.script(tacotron2)

    texts = []
    try:
        f = open(args.input, 'r')
        texts = f.readlines()
    except:
        print("Could not read file")
        sys.exit(1)

    with open('config.json') as f:
        audio_config = json.load(f)
    
    loader = MelLoader(text_cleaners=['english_cleaners'], 
                       max_wav_value=audio_config['audio']['max-wav-value'], 
                       sampling_rate=audio_config['audio']['sampling-rate'], 
                       filter_length=audio_config['audio']['filter-length'], 
                       hop_length=audio_config['audio']['hop-length'], 
                       win_length=audio_config['audio']['win-length'], 
                       n_mel_channels=80, 
                       mel_fmin=audio_config['audio']['mel-fmin'], 
                       mel_fmax=audio_config['audio']['mel-fmax'])

    # load emotion mels
    for emo, path in tag2ref.items():
        emo_mel = loader.get_mel(path)
        emo_mel = emo_mel.unsqueeze(0)

        if not args.cpu:
            emo_mel = emo_mel.to('cuda')

        tag2ref[emo] = emo_mel

    measurements = {}

    for i, text in enumerate(texts):
        print(text)
        sequences_padded, input_lengths, _ = prepare_input_sequence([text], args.cpu)
        
        emotions = [{'<Neutral>': {'mel': tag2ref['<Neutral>']},
                     '<Angry>': {'mel': tag2ref['<Angry>']},
                     '<Happy>': {'mel': tag2ref['<Happy>']},
                     '<Sad>': {'mel': tag2ref['<Sad>']},
                     '<Surprised>': {'mel': tag2ref['<Surprised>']}}]

        with torch.no_grad(), MeasureTime(measurements, "tacotron2_time", args.cpu):
            mel, mel_lengths, alignments = tacotron2(sequences_padded, input_lengths, emotions)

        with torch.no_grad(), MeasureTime(measurements, "waveglow_time", args.cpu):
            audios = jitted_waveglow(mel, sigma=args.sigma_infer)
            audios = audios.float()
        with torch.no_grad(), MeasureTime(measurements, "denoiser_time", args.cpu):
            audios = denoiser(audios, strength=args.denoising_strength).squeeze(1)

        print("Stopping after",mel.size(2),"decoder steps")
        tacotron2_infer_perf = mel.size(0)*mel.size(2)/measurements['tacotron2_time']
        waveglow_infer_perf = audios.size(0)*audios.size(1)/measurements['waveglow_time']

        DLLogger.log(step=0, data={"tacotron2_items_per_sec": tacotron2_infer_perf})
        DLLogger.log(step=0, data={"tacotron2_latency": measurements['tacotron2_time']})
        DLLogger.log(step=0, data={"waveglow_items_per_sec": waveglow_infer_perf})
        DLLogger.log(step=0, data={"waveglow_latency": measurements['waveglow_time']})
        DLLogger.log(step=0, data={"denoiser_latency": measurements['denoiser_time']})
        DLLogger.log(step=0, data={"latency": (measurements['tacotron2_time']+measurements['waveglow_time']+measurements['denoiser_time'])})

        assert len(audios) == 1
        audio = audios[0]
        plt.imshow(alignments[0].float().data.cpu().numpy().T, aspect="auto", origin="lower")
        figure_path = os.path.join(args.output,"alignment_"+str(i)+args.suffix+".png")
        plt.savefig(figure_path)

        audio = audio[:mel_lengths[0]*args.stft_hop_length]
        audio = audio/torch.max(torch.abs(audio))
        audio_path = os.path.join(args.output,"audio_"+str(i)+args.suffix+".wav")
        write(audio_path, args.sampling_rate, audio.cpu().numpy())

    DLLogger.flush()


if __name__ == '__main__':
    main_gst_by_one()
